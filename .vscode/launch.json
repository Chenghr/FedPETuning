{
    // 使用 IntelliSense 了解相关属性。 
    // 悬停以查看现有属性的描述。
    // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Debug: centralized-roberta",
            "type": "debugpy",
            "request": "launch",
            "program": "main.py", // ${file}
            "console": "integratedTerminal",
            "python": "${command:python.interpreterPath}",
            "args": [
                // datasets config
                "--task_name", "rte",
                "--dataset_name", "glue",
                "--raw_dataset_path", "./data/fedglue",
                "--partition_dataset_path", "./data/fedglue",
                "--max_seq_length", "128",
                // federated config
                "--fl_algorithm", "centralized",
                "--rank", "-1",
                "--pson", "true",
                "--test_rounds", "true",
                // models config
                "--model_name_or_path", "./pretrain/nlp/roberta-base/",
                "--model_type", "roberta-base",
                "--model_output_mode", "seq_classification", 
                "--tuning_type", "lora_roberta-base", 
                "--lora_alpha", "16", 
                // trainers config
                "--metric_name", "glue",
                "--do_grid", "False",
                // other config
                "--output_dir", "./output/fedglue",
                "--learning_rate", "0.001",
                "--seed", "1", 
            ],
            // The code is not customized for multi-GPU scenarios
            "env": {"CUDA_VISIBLE_DEVICES": "0"}
        },
        {
            "name": "Debug: centralized-llama",
            "type": "debugpy",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "python": "${command:python.interpreterPath}",
            "args": [
                // datasets config
                "--task_name", "rte",
                "--dataset_name", "glue",
                "--raw_dataset_path", "./data/fedglue",
                "--partition_dataset_path", "./data/fedglue",
                "--max_seq_length", "128",

                // federated config
                "--fl_algorithm", "centralized",
                "--rank", "-1",
                "--rounds", "3",
                "--test_rounds", "true",

                // models config
                "--model_name_or_path", "./pretrain/nlp/llama/llama-7b",
                "--model_type", "llama",
                "--model_output_mode", "seq_classification", 

                // trainers config
                // "--metric_name", "glue",
                "--tuning_library", "peft",
                "--tuning_type", "lora_llama",
                "--lora_r", "8",
                "--lora_alpha", "8", 
                "--do_grid", "False",
                // "--learning_rate", "0.001",
                // other config
                "--output_dir", "./output/fedglue",
                // "--seed", "1", 
            ],
            // The code is not customized for multi-GPU scenarios
            "env": {"CUDA_VISIBLE_DEVICES": "0"}
        }
    ],
}