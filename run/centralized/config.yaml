data_config:
  # dataset_name: glue
  # dataset_name: ner
  # raw_dataset_path: ./data/fedglue
  # partition_dataset_path: ./data/fedglue

federated_config:
  fl_algorithm: centralized
  # clients_num: 100
  # rounds: 100
  # alpha: 1.0
  # sample: 0.1
  rank: -1
  pson: true  # flase: not need for local test

model_config:
  # model_type: llama 
  # model_type: roberta
  # model_output_mode: seq_classification
  # model_output_mode: token_classification  
  # permutation_layers: false
  # client_model_layers: [0,1,2]
  # server_model_layers: [0,1,2]
  # tuning_type: lora_llama
  # tuning_type: adapter_roberta-base
  # tuning_type: soft_prompt_roberta-base
  # tuning_type: lora_roberta-base
  # tuning_type: bitfit_robert-base
  # tuning_type: prefix_robert-base

training_config:
  metric_name: glue
  # metric_name: conll
  num_train_epochs: 2
  per_device_train_batch_size: 32
  learning_rate: 2e-5
  # learning_rate: 5e-5
  do_predict: true